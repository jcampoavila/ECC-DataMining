{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SCRIPT TO BUILD DECISION TREE MODEL\n",
    "Generation of a model using the entire available dataset<br>\n",
    "Used best hyperparameter configuration in article (max_depth=4; min_samples_per_leaf=7 <br>\n",
    "Plots file (png type) is created<br>\n",
    "Rules file (excel type) is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version and rest of packages needed\n",
    "\n",
    "import sys\n",
    "# print('Python: {}'.format(sys.version))\n",
    "import scipy\n",
    "# print('scipy: {}'.format(scipy.__version__))\n",
    "import numpy as np\n",
    "# print('numpy: {}'.format(np.__version__))\n",
    "import matplotlib as mat\n",
    "# print('matplotlib: {}'.format(mat.__version__))\n",
    "import pandas as pd\n",
    "# print('pandas: {}'.format(pd.__version__))\n",
    "import sklearn as sk\n",
    "# print('sklearn: {}'.format(sk.__version__))\n",
    "import pyreadstat\n",
    "# print('pyreadstat: {}'.format(pyreadstat.__version__))\n",
    "import imblearn as im\n",
    "# print('imblearn: {}'.format(im.__version__))\n",
    "import joblib\n",
    "# print('joblib: {}'.format(joblib.__version__))\n",
    "import graphviz\n",
    "# print('graphviz: {}'.format(graphviz.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from packages\n",
    "\n",
    "from collections import Counter\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text, export_graphviz\n",
    "from sklearn.tree import _tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and visualize dataset\n",
    "\n",
    "df = pd.read_csv('../../INPUT_dataset/BDsocioeconomic_dummy.csv',delimiter=',',low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate columns\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some statistics about imbalanced class\n",
    "suicidal_behaviour_rate = df.loc[df['Class_suicidal_behaviour']==1].shape[0]/df.shape[0]*100\n",
    "non_suicidal_behaviour_rate = df.loc[df['Class_suicidal_behaviour']==0].shape[0]/df.shape[0]*100\n",
    "n_non_yes_suicidal_behaviour = Counter(df['Class_suicidal_behaviour'])\n",
    "print('The rate of non suicidal behaviour (0) is: {:.2f}%'.format(non_suicidal_behaviour_rate))\n",
    "print('The rate of suicidal behaviour (1) is: {:.2f}%'.format(suicidal_behaviour_rate))\n",
    "print('Number of instances of each class:')\n",
    "print(n_non_yes_suicidal_behaviour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise an instance to see what it looks like\n",
    "first_instance = df.iloc[0].drop(columns=['Class_suicidal_behaviour'])\n",
    "first_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation of a model using the entire available dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of the original dataset\n",
    "dataset = df.copy()   \n",
    "# dataset with the observations. Independent variables without the class\n",
    "df_values = dataset.drop(columns=['Class_suicidal_behaviour'])\n",
    "dataset_values = df_values.copy()   \n",
    "\n",
    "# dataset with the classes. Dependent variable\n",
    "df_targets = pd.DataFrame(dataset['Class_suicidal_behaviour'], columns=['Class_suicidal_behaviour'])\n",
    "dataset_targets = df_targets.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the importance of classes.\n",
    "# The cost of making a mistake missing a suicidal behaviour is higher (False Negative are worse than False Positive)\n",
    "class_weight = {0:0.2, 1:0.8}\n",
    "\n",
    "# seed for control randomized algorithm\n",
    "seed = 1\n",
    "\n",
    "# Setting of the decision tree learner algorithm\n",
    "rebalanced_model = DecisionTreeClassifier(# max_leaf_nodes=xx, # min_samples_split=xx,\n",
    "                                            max_depth=4, min_samples_leaf=7,                                                                                           \n",
    "                                            class_weight=class_weight, random_state=seed)\n",
    "# second instance of the result becaUse later the error with original data will be calculated\n",
    "original_data_model = DecisionTreeClassifier(# max_leaf_nodes=xx, # min_samples_split=xx,\n",
    "                                             max_depth=4, min_samples_leaf=7,                                                                                           \n",
    "                                             class_weight=class_weight, random_state=seed)\n",
    "\n",
    "# for debugging results in every iteration (0=none, 1=low, 2=high)\n",
    "debug_level = 0\n",
    "         \n",
    "# some information\n",
    "if debug_level>=1:\n",
    "    suicidal_behaviour_rate = dataset_targets.loc[dataset_targets['Class_suicidal_behaviour']==1].shape[0]/dataset_targets.shape[0]*100\n",
    "    n_non_yes_suicidal_behaviour = Counter(dataset_targets['Class_suicidal_behaviour'])\n",
    "    print('The rate of suicidal behaviour (1) BEFORE rebalancing is: {:.2f}%'.format(suicidal_behaviour_rate))\n",
    "    print('Number of instances of each class')\n",
    "    print(n_non_yes_suicidal_behaviour)      \n",
    "\n",
    "# to do the rebalancing, it workS better if everything is scaled between 0 and 1\n",
    "this_scaler = preprocessing.MinMaxScaler()\n",
    "this_scaler = this_scaler.fit(dataset_values)\n",
    "dataset_values = this_scaler.transform(dataset_values)\n",
    "\n",
    "# we rebalance the dataset\n",
    "oversampling_conf = SMOTE(sampling_strategy=0.1,random_state=seed)\n",
    "undersamplinf_conf = RandomUnderSampler(sampling_strategy=0.2,random_state=seed)\n",
    "rebalance_steps = [('o', oversampling_conf), ('u', undersamplinf_conf)]\n",
    "rebalance_pipeline = Pipeline(steps=rebalance_steps)\n",
    "dataset_values, dataset_targets = rebalance_pipeline.fit_resample(dataset_values, dataset_targets)\n",
    "\n",
    "# This model does not need normalised data\n",
    "# so we return the data to their original values\n",
    "dataset_values = this_scaler.inverse_transform(dataset_values)\n",
    "\n",
    "# In models where the data does NOT have to be normalised\n",
    "# rounding columns that had INTEGER values can help produce a better model\n",
    "dataset_values[:,0]  = list(map(round, dataset_values[:,0]))    # Sex_M0_F1\n",
    "dataset_values[:,2]  = list(map(round, dataset_values[:,2]))    # Day_in_week\n",
    "dataset_values[:,3]  = list(map(round, dataset_values[:,3]))    # Day_in_month\n",
    "dataset_values[:,4]  = list(map(round, dataset_values[:,4]))    # Month\n",
    "dataset_values[:,5]  = list(map(round, dataset_values[:,5]))    # Quarter\n",
    "dataset_values[:,6]  = list(map(round, dataset_values[:,6]))    # Week_in_year\n",
    "dataset_values[:,7]  = list(map(round, dataset_values[:,7]))    # Week_in_month\n",
    "dataset_values[:,8]  = list(map(round, dataset_values[:,8]))    # Working_day\n",
    "dataset_values[:,9]  = list(map(round, dataset_values[:,9]))    # Day1_Night2\n",
    "dataset_values[:,17] = list(map(round, dataset_values[:,17]))   # Num_requests_last_months\n",
    "\n",
    "# some information\n",
    "if debug_level>=1:\n",
    "    suicidal_behaviour_rate = dataset_targets.loc[dataset_targets['Class_suicidal_behaviour']==1].shape[0]/dataset_targets.shape[0]*100\n",
    "    n_non_yes_suicidal_behaviour = Counter(dataset_targets['Class_suicidal_behaviour'])\n",
    "    print('The rate of suicidal behaviour (1) AFTER rebalancing is: {:.2f}%'.format(suicidal_behaviour_rate))\n",
    "    print('Number of instances of each class')\n",
    "    print(n_non_yes_suicidal_behaviour)    \n",
    "    \n",
    "    \n",
    "# we train the model with the rebalanced datasets\n",
    "rebalanced_model.fit(dataset_values, np.ravel(dataset_targets))\n",
    "\n",
    "# we make a copy of the model because later it is going to be used to calculate the error with original dataset\n",
    "original_data_model.fit(dataset_values, np.ravel(dataset_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT trees\n",
    "Generated with rebalanced dataset<br>\n",
    "Generated with rebalanced dataset and relabeled with original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the tree to png\n",
    "dot_data = export_graphviz(rebalanced_model, out_file=None, \n",
    "                            feature_names=df_values.columns,  \n",
    "                            class_names=['Non_suicidal','Suicidal'],\n",
    "                            filled=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_rebalanced_data_do_NOT_USE_hyperparameter_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model with real information in the original dataset\n",
    "\n",
    "# for debugging results in every iteration (0=none, 1=low, 2=high)\n",
    "debug_level = 1\n",
    "\n",
    "# Keep the structure, but remove the counters\n",
    "for i in range (0,original_data_model.tree_.node_count):\n",
    "    original_data_model.tree_.value[i] = [[0.,0.]]\n",
    "    original_data_model.tree_.impurity[i] = 0\n",
    "    original_data_model.tree_.n_node_samples[i] = 0\n",
    "    original_data_model.tree_.weighted_n_node_samples[i] = 1.\n",
    "    \n",
    "# Fill the nodes with the values of the data without rebalancing (original data)\n",
    "# Currently is done including, one by one, every instance in the tree\n",
    "# A more advanced procedure can be proposed in the future\n",
    "\n",
    "# some code is used to debug the evolution of the process\n",
    "\n",
    "if debug_level>=1:\n",
    "    percentageInt_printed = 0\n",
    "\n",
    "# for every instance\n",
    "for i in range(0,len(df_values)):\n",
    "    if debug_level>=1:\n",
    "        percentageFloat = ((i+1)/(len(df_values)+1))*100\n",
    "        percentageInt = round(percentageFloat)\n",
    "        if percentageInt_printed != percentageInt:\n",
    "            percentageInt_printed = percentageInt\n",
    "            percentageBar_printed = round(percentageInt_printed / 2)\n",
    "            sys.stdout.write('\\r')\n",
    "            # the exact output you're looking for:\n",
    "            sys.stdout.write(\"[%-50s] %.2f%%\" % ('='*percentageBar_printed, percentageInt_printed))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    # get the path corresponding to the instance\n",
    "    node_indicator = original_data_model.decision_path(df_values.iloc[i].to_numpy().reshape(1, -1))\n",
    "    # for every node that matches the instance in the path\n",
    "    for j in node_indicator.indices:\n",
    "        original_data_model.tree_.value[j][0][df_targets.iloc[i]] += 1\n",
    "        original_data_model.tree_.n_node_samples[j] += 1\n",
    "        original_data_model.tree_.weighted_n_node_samples[j] += 1        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the tree to png\n",
    "dot_data = export_graphviz(original_data_model, out_file=None, \n",
    "                            feature_names=df_values.columns,  \n",
    "                            class_names=['Non_suicidal','Suicidal'],\n",
    "                            filled=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_original_data_hyperparameter_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT RULES TO EXCEL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the rules in text\n",
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        \n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "            p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [f\"({name} > {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    count = 0\n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        count = count + 1\n",
    "        rule = \"if \"     \n",
    "        for p in path[:-1]:\n",
    "            if rule != \"if \":\n",
    "                rule += \"\\n and \"\n",
    "            rule += str(p)\n",
    "        rule += \"\\n then \"\n",
    "        if class_names is None:\n",
    "            rule += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            if np.sum(classes) == 0:\n",
    "                rule += f\"class: {class_names[l]} (proba: 0%)\"\n",
    "            else:\n",
    "                rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "        rule += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule += \"\\n--------------------------------------------------------------\"\n",
    "        rule += \"\\n\"\n",
    "        rules += [rule]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the rules from the tree trained with rebalanced dataset\n",
    "# TEXT MODE =  DEBUG_LEVEL = 1\n",
    "rules = get_rules(rebalanced_model, df_values.columns, ['Non_suicidal','Suicidal'])\n",
    "\n",
    "print('Rules: ', len(rules))\n",
    "# for debugging results in every iteration (0=none, 1=low, 2=high)\n",
    "debug_level = 0\n",
    "         \n",
    "# some information\n",
    "if debug_level>=1:\n",
    "    print('')\n",
    "    for r in rules:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the rules from the tree trained with rebalanced dataset,\n",
    "# BUT relabelled with original data.\n",
    "# TEXT MODE =  DEBUG_LEVEL = 1\n",
    "rules = get_rules(original_data_model, df_values.columns, ['Non_suicidal','Suicidal'])\n",
    "\n",
    "print('Rules: ', len(rules))\n",
    "# for debugging results in every iteration (0=none, 1=low, 2=high)\n",
    "debug_level = 0\n",
    "         \n",
    "# some information\n",
    "if debug_level>=1:\n",
    "    print('')\n",
    "    for r in rules:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the above function to extract the rules.\n",
    "# but reconverted to reformat them into an excel file\n",
    "def get_rules_EXCEL(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    \n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    total = tree_.node_count # Number of nodes\n",
    "    tsamples = tree_.n_node_samples[0]\n",
    "    \n",
    "    # To check how many times an attribute has been used in all the rules\n",
    "    useAttributes = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "   \n",
    "    rawData  = list() # For data\n",
    "    rawRules = list() # For rules\n",
    "    amIaLeaf = list() # To check which nodes are leaves\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED: # If NOT leaf\n",
    "            if path: # It will not run on the root, on the rest of the intermediate nodes it will.\n",
    "                tempPath = path.copy()\n",
    "                tempPath += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "                paths += [tempPath]\n",
    "                amIaLeaf.append('False')\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "            p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_left[node], p1, paths)  # Left node recursion\n",
    "            p2 += [f\"({name} > {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_right[node], p2, paths) # Rigth node recursion\n",
    "        else: # If YES leaf\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            amIaLeaf.append('True')\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "  \n",
    "    count = -1\n",
    "    \n",
    "    for path in paths:\n",
    "        count = count + 1\n",
    "        rule = \"if \"\n",
    "        \n",
    "        # The values of each row of the spreadsheet\n",
    "        rowValues = [None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,\n",
    "                 None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]\n",
    "        \n",
    "        # Set to 1 if the attribute appears in the rule.\n",
    "        useLocalAttributes = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] \n",
    "        \n",
    "        # Metadata\n",
    "        extraValues = [0,0,0,0,0,False] \n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            if rule != \"if \":\n",
    "                rule += \"\\n and \"\n",
    "            rule += str(p)\n",
    "            words = str(p).split()\n",
    "            value = float(words[-1].replace(\")\", \"\"))\n",
    "            \n",
    "            # Values of each field\n",
    "            if 'Sex_M0_F1' in str(p):\n",
    "                useLocalAttributes[0] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[0] = 'F'\n",
    "                else:\n",
    "                    rowValues[0] = 'M'\n",
    "            elif 'Age' in str(p):\n",
    "                useLocalAttributes[1] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[1] = value\n",
    "                else:\n",
    "                    rowValues[2] = value       \n",
    "            elif 'Day_in_week' in str(p):\n",
    "                useLocalAttributes[2] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[3] = value\n",
    "                else:\n",
    "                    rowValues[4] = value\n",
    "            elif 'Day_in_month' in str(p):\n",
    "                useLocalAttributes[3] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[5] = value\n",
    "                else:\n",
    "                    rowValues[6] = value\n",
    "            elif 'Month' in str(p):\n",
    "                useLocalAttributes[4] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[7] = value\n",
    "                else:\n",
    "                    rowValues[8] = value\n",
    "            elif 'Quarter' in str(p):\n",
    "                useLocalAttributes[5] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[9] = value\n",
    "                else:\n",
    "                    rowValues[10] = value\n",
    "            elif 'Week_in_year' in str(p):\n",
    "                useLocalAttributes[6] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[11] = value\n",
    "                else:\n",
    "                    rowValues[12] = value\n",
    "            elif 'Week_in_month' in str(p):\n",
    "                useLocalAttributes[7] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[13] = value\n",
    "                else:\n",
    "                    rowValues[14] = value\n",
    "            elif 'Working_day' in str(p):\n",
    "                useLocalAttributes[8] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[15] = value\n",
    "                else:\n",
    "                    rowValues[16] = value\n",
    "            elif 'Day1_Night2' in str(p):\n",
    "                useLocalAttributes[9] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[17] = value\n",
    "                else:\n",
    "                    rowValues[18] = value\n",
    "            elif 'Population_with_income_per_unit_of_consumption_below_50perc_of_median' in str(p):\n",
    "                useLocalAttributes[10] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[19] = value\n",
    "                else:\n",
    "                    rowValues[20] = value\n",
    "            elif 'Population_with_income_per_consumption_unit_below_5000_Euros' in str(p):\n",
    "                useLocalAttributes[11] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[21] = value\n",
    "                else:\n",
    "                    rowValues[22] = value\n",
    "            elif 'Percentage_of_single_person_households' in str(p):\n",
    "                useLocalAttributes[12] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[23] = value\n",
    "                else:\n",
    "                    rowValues[24] = value\n",
    "            elif 'Percentage_of_population_aged_65_and_over' in str(p):\n",
    "                useLocalAttributes[13] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[25] = value\n",
    "                else:\n",
    "                    rowValues[26] = value\n",
    "            elif 'Percentage_of_population_under_18' in str(p):\n",
    "                useLocalAttributes[14] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[27] = value\n",
    "                else:\n",
    "                    rowValues[28] = value\n",
    "            elif 'Average_net_income_per_person' in str(p):\n",
    "                useLocalAttributes[15] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[29] = value\n",
    "                else:\n",
    "                    rowValues[30] = value\n",
    "            elif 'Average_household_size' in str(p):\n",
    "                useLocalAttributes[16] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[31] = value\n",
    "                else:\n",
    "                    rowValues[32] = value\n",
    "            elif 'Num_requests_last_months' in str(p):\n",
    "                useLocalAttributes[17] = 1\n",
    "                if '>' in str(p):\n",
    "                    rowValues[33] = value\n",
    "                else:\n",
    "                    rowValues[34] = value\n",
    "        \n",
    "        useAttributes = np.add(useAttributes, useLocalAttributes) # We update the use of global attributes\n",
    "        extraValues[0] = path[-1][1] # Number of samples\n",
    "        extraValues[1] = path[-1][1]/tsamples # Percentage that the samples represent out of all samples\n",
    "        if np.sum(path[-1][0][0])==0:\n",
    "            extraValues[2] = 0\n",
    "        else: \n",
    "            extraValues[2] = path[-1][0][0][1]/np.sum(path[-1][0][0]) # Percentage of suicidal behaviour\n",
    "        extraValues[3] = len(path[:-1]) # Depth of the rule\n",
    "        extraValues[4] = sum(useLocalAttributes) # How many different attributes are used in the rule\n",
    "        extraValues[5] = amIaLeaf[count] # Whether it is a leaf or not\n",
    "        extraValues.extend(rowValues)\n",
    "        \n",
    "        rawData.append(extraValues)\n",
    "        \n",
    "        rule += \"\\n then \"\n",
    "        if class_names is None:\n",
    "            rule += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)           \n",
    "            if np.sum(classes) == 0:\n",
    "                rule += f\"class: {class_names[l]} (proba: 0%)\"\n",
    "            else:\n",
    "                rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "            # rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "        rule += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rawRules.append(rule) \n",
    "\n",
    "    # The rules   \n",
    "    df1 = pd.DataFrame(rawData, columns=['Samples','Coverage','SuicidePercentage','Depth','AttributesUsed','Leaf',\n",
    "                              'Sex_M0_F1','Age_Inf','Age_Sup','Day_in_week_Inf','Day_in_week_Sup',\n",
    "                              'Day_in_month_Inf','Day_in_month_Sup','Month_Inf','Month_Sup','Quarter_Inf','Quarter_Sup',\n",
    "                              'Week_in_year_Inf','Week_in_year_Sup','Week_in_month_Inf',\n",
    "                              'Week_in_month_Sup','Working_day_Inf','Working_day_Sup','Day1_Night2_Inf','Day1_Night2_Sup',\n",
    "                              'Population_with_income_per_unit_of_consumption_below_50perc_of_median_Inf',\n",
    "                              'Population_with_income_per_unit_of_consumption_below_50perc_of_mediana_Sup',\n",
    "                              'Population_with_income_per_consumption_unit_below_5000_Euros_Inf',\n",
    "                              'Population_with_income_per_consumption_unit_below_5000_Euros_Sup',\n",
    "                              'Percentage_of_single_person_households_Inf','Percentage_of_single_person_households_Sup',\n",
    "                              'Percentage_of_population_aged_65_and_over_Inf','Percentage_of_population_aged_65_and_over_Sup',\n",
    "                              'Percentage_of_population_under_18_Inf','Percentage_of_population_under_18_Sup',\n",
    "                              'Average_net_income_per_person_Inf','Average_net_income_per_person_Sup',\n",
    "                              'Average_household_size_Inf','Average_household_size_Sup',\n",
    "                              'Num_requests_last_months_Inf','Num_requests_last_months_Sup'])\n",
    "    df2 = pd.DataFrame(rawRules, columns=['Rule']) # Rule in original text format\n",
    "    df3 = pd.DataFrame([useAttributes], columns=list(df_values.columns)) # Use of global attributes\n",
    "       \n",
    "    return df1,df2,df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run the function with our transformed tree.\n",
    "df1,df2,df3 = get_rules_EXCEL(rebalanced_model, df_values.columns, ['Non_suicidal','Suicidal'])\n",
    "\n",
    "with pd.ExcelWriter('RULES_decision_tree_rebalanced_data_do_NOT_USE_hyperparameter_best.xlsx') as writer:  # Export to excel\n",
    "    df1.to_excel(writer, sheet_name='Rule_Details')\n",
    "    df2.to_excel(writer, sheet_name='Raw_Rules')\n",
    "    df3.to_excel(writer, sheet_name='Attribute_Count')\n",
    "\n",
    "# for debugging results in every iteration (0=none, 1=low, 2=high)\n",
    "debug_level = 0\n",
    "         \n",
    "# some information\n",
    "if debug_level>=1:\n",
    "    print('')\n",
    "    for r in rules:\n",
    "        print(r)\n",
    "\n",
    "# Total number of nodes (all) and rules (leaf nodes)\n",
    "print('Nodes: ',rebalanced_model.tree_.node_count)\n",
    "print('Rules: ',Counter(rebalanced_model.tree_.feature)[_tree.TREE_UNDEFINED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run the function with our transformed tree.\n",
    "df1,df2,df3 = get_rules_EXCEL(original_data_model, df_values.columns, ['Non_suicidal','Suicidal'])\n",
    "\n",
    "with pd.ExcelWriter('RULES_decision_tree_original_data_hyperparameter_best.xlsx') as writer:  # Export to excel\n",
    "    df1.to_excel(writer, sheet_name='Rule_Details')\n",
    "    df2.to_excel(writer, sheet_name='Raw_Rules')\n",
    "    df3.to_excel(writer, sheet_name='Attribute_Count')\n",
    "\n",
    "# for debugging results in every iteration (0=none, 1=low, 2=high)\n",
    "debug_level = 0\n",
    "         \n",
    "# some information\n",
    "if debug_level>=1:\n",
    "    print('')\n",
    "    for r in rules:\n",
    "        print(r)\n",
    "\n",
    "# Total number of nodes (all) and rules (leaf nodes)\n",
    "print('Nodes: ',original_data_model.tree_.node_count)\n",
    "print('Rules: ',Counter(original_data_model.tree_.feature)[_tree.TREE_UNDEFINED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
